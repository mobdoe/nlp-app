{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Summarizer [product]",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNNrck9hWSQ4q+1oFuwiM0R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobdoe/nlp-app/blob/master/Text_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQMRszkEjgVG",
        "colab_type": "text"
      },
      "source": [
        "1. RUN FIRST \n",
        "2. RESTART RUNTIME\n",
        "3. RUN ALL\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03fbuPPaYqap",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cf21e6c8-bded-408c-df45-275e6aaf078b"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.6/dist-packages (0.66.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from streamlit) (2.8.1)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (5.1.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (from streamlit) (0.14.1)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.6/dist-packages (from streamlit) (0.5.0b1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.6/dist-packages (from streamlit) (2.0.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from streamlit) (1.14.59)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from streamlit) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (7.0.0)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (4.1.0)\n",
            "Requirement already satisfied: botocore>=1.13.44 in /usr/local/lib/python3.6/dist-packages (from streamlit) (1.17.59)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (3.12.4)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.6/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from streamlit) (0.10.1)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (1.0.5)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (4.1.1)\n",
            "Requirement already satisfied: enum-compat in /usr/local/lib/python3.6/dist-packages (from streamlit) (0.0.3)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.6/dist-packages (from streamlit) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from streamlit) (20.4)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.6/dist-packages (from streamlit) (0.8.1)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.6/dist-packages (from streamlit) (0.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->streamlit) (1.15.0)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.5.1)\n",
            "Requirement already satisfied: jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit) (2.11.2)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit) (4.3.3)\n",
            "Requirement already satisfied: ipykernel>=5.1.2; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.3.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from tzlocal->streamlit) (2018.9)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->streamlit) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->streamlit) (0.3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->streamlit) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->streamlit) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->streamlit) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->streamlit) (1.24.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit) (0.10.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore>=1.13.44->streamlit) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.0->streamlit) (50.3.0)\n",
            "Requirement already satisfied: pathtools>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from watchdog->streamlit) (0.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->streamlit) (2.4.7)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from validators->streamlit) (4.4.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.5.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.10.1->pydeck>=0.1.dev5->streamlit) (1.1.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.6.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (19.0.2)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.6.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.4.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.1.5)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DSoMSlhWIhM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "2a50ca52-e0a4-442e-9f39-906a0454fa72"
      },
      "source": [
        "!pip install pyngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/4e/c050f5ed75f60cca4e9cc0a7056a9b424707a09f6293756e7dd19beb10db/pyngrok-4.1.12.tar.gz\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyngrok) (0.16.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyngrok) (3.13)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-4.1.12-cp36-none-any.whl size=16810 sha256=e1220af925876f9b074545a2d7c8ff17a26e8de7da294f0f27970f6e4f299eca\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/c3/d6/6968dd4d831794d41c311be1d7af6f4ac151c5d3bd0e6efab8\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-4.1.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20pMOKMORAtm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13qRqBDgQ9g5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d9153055-29e1-4769-cfb2-dd17ebb510a1"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nHqhtkASnvY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "0a64f756-c390-4310-b42f-c3bc8a7ae3d2"
      },
      "source": [
        "! wget http://nlp.stanford.edu/data/glove.6B.zip # download glove\n",
        "! unzip glove*.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-15 18:45:12--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-09-15 18:45:13--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-09-15 18:45:13--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.47MB/s    in 9m 55s  \n",
            "\n",
            "2020-09-15 18:55:19 (1.38 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy-ug25-dZke",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 996
        },
        "outputId": "a13f00cd-87a3-4927-992f-6e0e0b49a5e3"
      },
      "source": [
        "!pip install bert-extractive-summarizer # download bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-extractive-summarizer\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/f3/b0cb3328d9fa41e5f430f491f6558ada4366190a38ce91bbb881195492d7/bert-extractive-summarizer-0.5.0.tar.gz\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from bert-extractive-summarizer) (0.22.2.post1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from bert-extractive-summarizer) (2.2.4)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 14.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (2019.12.20)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 34.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 47.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (2.23.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->bert-extractive-summarizer) (0.16.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (3.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (0.8.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (50.3.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (2.0.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers->bert-extractive-summarizer) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->bert-extractive-summarizer) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->bert-extractive-summarizer) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->bert-extractive-summarizer) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->bert-extractive-summarizer) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.1.0)\n",
            "Building wheels for collected packages: bert-extractive-summarizer, sacremoses\n",
            "  Building wheel for bert-extractive-summarizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-extractive-summarizer: filename=bert_extractive_summarizer-0.5.0-cp36-none-any.whl size=16417 sha256=98f4e40b530dd9086ae3af35d3605bf4b7fbd4d1015e45444d07e41fb3caf306\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/89/f7/13d71ca6b8e825191f2c5bfbe91f425cf12d2eeca07935cf13\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=4a69dd71daa96ff7442f1dcea803ea1f7de91fe8df619876f445c36190150a37\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built bert-extractive-summarizer sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers, bert-extractive-summarizer\n",
            "Successfully installed bert-extractive-summarizer-0.5.0 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BMnMoi5eQhC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c93904b3-b217-4586-a4fa-1578479ef8d4"
      },
      "source": [
        "#! pip install spacy==2.2.2\n",
        "! pip install transformers==2.2.2\n",
        "! pip install neuralcoref"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==2.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/08/4a6768ca1a7a4fa37e5ee08077c5d02b8d83876bd36caa5fc24d98992ac2/transformers-2.2.2-py3-none-any.whl (387kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (1.14.59)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (0.0.43)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (2019.12.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.2.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.2.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.2.2) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.2.2) (1.24.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.59 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.2.2) (1.17.59)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.2.2) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.2.2) (0.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.2.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.2.2) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.2.2) (1.15.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.59->boto3->transformers==2.2.2) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.59->boto3->transformers==2.2.2) (2.8.1)\n",
            "Installing collected packages: transformers\n",
            "  Found existing installation: transformers 3.1.0\n",
            "    Uninstalling transformers-3.1.0:\n",
            "      Successfully uninstalled transformers-3.1.0\n",
            "Successfully installed transformers-2.2.2\n",
            "Collecting neuralcoref\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/24/0ec7845a5b73b637aa691ff4d1b9b48f3a0f3369f4002a59ffd7a7462fdb/neuralcoref-4.0-cp36-cp36m-manylinux1_x86_64.whl (287kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (1.14.59)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (1.18.5)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (2.2.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.59 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref) (1.17.59)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref) (0.10.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.8.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (50.3.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.2)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.59->boto3->neuralcoref) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.59->boto3->neuralcoref) (2.8.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->neuralcoref) (1.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.59->boto3->neuralcoref) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->neuralcoref) (3.1.0)\n",
            "Installing collected packages: neuralcoref\n",
            "Successfully installed neuralcoref-4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtTTJUvgeEYC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "40465f94-08de-43c1-b285-8acae8384296"
      },
      "source": [
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz --no-deps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
            "\u001b[K     |████████████████████████████████| 37.4MB 116kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.0.0-cp36-none-any.whl size=37405978 sha256=e498c9b4a0f4f20fc5a289d70e1e71dd77cad3e05d8383479f5ab158c987bffa\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/7c/d8/f86364af8fbba7258e14adae115f18dd2c91552406edc3fdaa\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQYQBNJ2nZOR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "c578a1e7-d24a-4d27-a49c-d49fc4b5b72b"
      },
      "source": [
        "! python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0MB 748kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.5-cp36-none-any.whl size=12011740 sha256=ce16054793f2bbc33e7421b1141ed60d72cd6151528c6d6962455c6f075955e6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nz80e5qc/wheels/6a/47/fb/6b5a0b8906d8e8779246c67d4658fd8a544d4a03a75520197a\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.0.0\n",
            "    Uninstalling en-core-web-sm-2.0.0:\n",
            "      Successfully uninstalled en-core-web-sm-2.0.0\n",
            "Successfully installed en-core-web-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1vyLHs2nTlu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "3c2e42fd-0af5-41ff-872f-a905846db089"
      },
      "source": [
        "!python -m spacy validate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r⠙ Loading compatibility table...\r\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
            "\u001b[1m\n",
            "====================== Installed models (spaCy v2.2.4) ======================\u001b[0m\n",
            "\u001b[38;5;4mℹ spaCy installation: /usr/local/lib/python3.6/dist-packages/spacy\u001b[0m\n",
            "\n",
            "TYPE      NAME             MODEL            VERSION                            \n",
            "package   en-core-web-sm   en_core_web_sm   \u001b[38;5;2m2.2.5\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
            "link      en               en_core_web_sm   \u001b[38;5;2m2.2.5\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9P3PU0AI6Na",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f856acd-c7a2-43ab-d8f7-8d3f8047a95f"
      },
      "source": [
        "%%writefile textsummarizer.py\n",
        "import streamlit as st\n",
        "PAGE_CONFIG = {\"page_title\":\"Text Summarizer | TMU\",\"page_icon\":\":smiley:\",\"layout\":\"centered\"}\n",
        "st.beta_set_page_config(**PAGE_CONFIG)\n",
        "\n",
        "# setting up environment\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "stopwords = set(STOPWORDS) \n",
        "from textblob import TextBlob \n",
        "from summarizer import Summarizer\n",
        "import spacy\n",
        "\n",
        "# WORD CLOUDS\n",
        "def wordcloud(text):\n",
        "  # Create and generate a word cloud image:\n",
        "  wordcloud = WordCloud(max_words=20, colormap=\"Blues\", stopwords=stopwords).generate(text)\n",
        "\n",
        "  # Display the generated image:\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n",
        "  st.pyplot()\n",
        "\n",
        "# ENTITY EXTRACTION\n",
        "def entityanalyzer(text):\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  sumx = nlp(text)\n",
        "  tokens = [token.text for token in sumx]\n",
        "  entities = [(entity.text,entity.label_) for entity in sumx.ents]\n",
        "  mylist = list(dict.fromkeys(entities))\n",
        "  return mylist\n",
        "\n",
        "# SUMMARIZATION METHODS\n",
        "# WORDFREQUENCY\n",
        "# setting up environment\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "def _create_frequency_table(text_string) -> dict: # create dictionary for word frequency table\n",
        "  words = word_tokenize(text_string)\n",
        "  ps = PorterStemmer()\n",
        "\n",
        "  freqTable = dict()\n",
        "  for word in words:\n",
        "      word = ps.stem(word)\n",
        "      if word in stopwords:\n",
        "          continue\n",
        "      if word in freqTable:\n",
        "          freqTable[word] += 1\n",
        "      else:\n",
        "          freqTable[word] = 1\n",
        "\n",
        "  return freqTable\n",
        "\n",
        "def _create_frequency_matrix(sentences): # create frequency matrix\n",
        "  frequency_matrix = {}\n",
        "  ps = PorterStemmer()\n",
        "\n",
        "  for sent in sentences:\n",
        "      freq_table = {}\n",
        "      words = word_tokenize(sent)\n",
        "      for word in words:\n",
        "          word = word.lower()\n",
        "          word = ps.stem(word)\n",
        "          if word in stopwords:\n",
        "            continue\n",
        "\n",
        "          if word in freq_table:\n",
        "              freq_table[word] += 1\n",
        "          else:\n",
        "              freq_table[word] = 1\n",
        "\n",
        "      frequency_matrix[sent[:15]] = freq_table\n",
        "\n",
        "  return frequency_matrix\n",
        "\n",
        "def _create_tf_matrix(freq_matrix): # create term frequency matrix\n",
        "  tf_matrix = {}\n",
        "\n",
        "  for sent, f_table in freq_matrix.items():\n",
        "      tf_table = {}\n",
        "\n",
        "      count_words_in_sentence = len(f_table)\n",
        "      for word, count in f_table.items():\n",
        "          tf_table[word] = count / count_words_in_sentence\n",
        "\n",
        "      tf_matrix[sent] = tf_table\n",
        "\n",
        "  return tf_matrix\n",
        "\n",
        "def _create_documents_per_words(freq_matrix): # table for documents\n",
        "    word_per_doc_table = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        for word, count in f_table.items():\n",
        "            if word in word_per_doc_table:\n",
        "                word_per_doc_table[word] += 1\n",
        "            else:\n",
        "                word_per_doc_table[word] = 1\n",
        "\n",
        "    return word_per_doc_table\n",
        "\n",
        "def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents): # idf matrix\n",
        "    import math\n",
        "    idf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        idf_table = {}\n",
        "\n",
        "        for word in f_table.keys():\n",
        "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
        "\n",
        "        idf_matrix[sent] = idf_table\n",
        "\n",
        "    return idf_matrix\n",
        "\n",
        "def _create_tf_idf_matrix(tf_matrix, idf_matrix): # tf=idf matrix\n",
        "    tf_idf_matrix = {}\n",
        "\n",
        "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
        "\n",
        "        tf_idf_table = {}\n",
        "\n",
        "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
        "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
        "            tf_idf_table[word1] = float(value1 * value2)\n",
        "\n",
        "        tf_idf_matrix[sent1] = tf_idf_table\n",
        "\n",
        "    return tf_idf_matrix\n",
        "\n",
        "def _score_sentences(tf_idf_matrix) -> dict: # sentence score\n",
        "\n",
        "    sentenceValue = {}\n",
        "\n",
        "    for sent, f_table in tf_idf_matrix.items():\n",
        "        total_score_per_sentence = 0\n",
        "\n",
        "        count_words_in_sentence = len(f_table)\n",
        "        for word, score in f_table.items():\n",
        "            total_score_per_sentence += score\n",
        "\n",
        "        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n",
        "\n",
        "    return sentenceValue\n",
        "\n",
        "def _find_average_score(sentenceValue) -> int: # find threshold\n",
        "\n",
        "    sumValues = 0\n",
        "    for entry in sentenceValue:\n",
        "        sumValues += sentenceValue[entry]\n",
        "\n",
        "    # Average value of a sentence from original summary_text\n",
        "    average = (sumValues / len(sentenceValue))\n",
        "\n",
        "    return average\n",
        "\n",
        "def _generate_summary(sentences, sentenceValue, threshold): # generate summary\n",
        "    sentence_count = 0\n",
        "    summary = ''\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence[:15] in sentenceValue and sentenceValue[sentence[:15]] >= (threshold):\n",
        "            summary += \" \" + sentence\n",
        "            sentence_count += 1\n",
        "\n",
        "    return summary\n",
        "\n",
        "def wordfreq(text):\n",
        "  sentences = sent_tokenize(text) # NLTK function\n",
        "  total_documents = len(sentences)\n",
        "\n",
        "  freq_matrix = _create_frequency_matrix(sentences)\n",
        "  tf_matrix = _create_tf_matrix(freq_matrix)\n",
        "  count_doc_per_words = _create_documents_per_words(freq_matrix)\n",
        "  idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
        "  tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
        "  sentence_scores = _score_sentences(tf_idf_matrix)\n",
        "  threshold = _find_average_score(sentence_scores)\n",
        "  summary = _generate_summary(sentences, sentence_scores, 1 * threshold)\n",
        "\n",
        "  return summary\n",
        "\n",
        "# TEXTRANK\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def remove_stopwords(sen):\n",
        "  sen_new = \" \".join([i for i in sen if i not in stopwords])\n",
        "  return sen_new\n",
        "\n",
        "def textrank(text):\n",
        "  sentences = []\n",
        "  sentences.append(sent_tokenize(text))\n",
        "  sentences = [y for x in sentences for y in x] # flatten list\n",
        "  clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \") # remove specials\n",
        "  clean_sentences = [s.lower() for s in clean_sentences] # lowercase\n",
        "  clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences] # remove stopwords\n",
        "\n",
        "  word_embeddings = {}\n",
        "  f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      word_embeddings[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  sentence_vectors = []\n",
        "  \n",
        "  for i in clean_sentences:\n",
        "    if len(i) != 0:\n",
        "      v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
        "    else:\n",
        "      v = np.zeros((100,))\n",
        "    sentence_vectors.append(v)\n",
        "  sim_mat = np.zeros([len(sentences), len(sentences)]) # similarity matrix\n",
        "  \n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  for i in range(len(sentences)):\n",
        "    for j in range(len(sentences)):\n",
        "      if i != j:\n",
        "        sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
        "  \n",
        "  import networkx as nx\n",
        "  nx_graph = nx.from_numpy_array(sim_mat)\n",
        "  scores = nx.pagerank(nx_graph)\n",
        "\n",
        "  ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "  # Specify number of sentences to form the summary\n",
        "  # sn = 3\n",
        "  # for i in range(sn):\n",
        "  summary = [i[1] for i in ranked_sentences]\n",
        "  top_5 = summary[:5]\n",
        "  space = \" \"\n",
        "  final_sum = space.join(top_5)\n",
        "  return final_sum\n",
        "\n",
        "# BERT SUMMARIZER\n",
        "def bert(text):\n",
        "  model = Summarizer()\n",
        "  summary = model(text)\n",
        "\n",
        "  return summary\n",
        "\n",
        "# WORD COUNTER\n",
        "def wordcount(text):\n",
        "  words = text.split(\" \")\n",
        "  num_words = len(words)\n",
        "  return num_words\n",
        "\n",
        "def main():\n",
        "\tst.title(\"Final Capstone | Thinkful\")\n",
        "\tst.subheader(\"Text Summarization\")\n",
        "\tst.markdown(\"\"\"\n",
        "    \t#### Description\n",
        "    \t+ This is a Natural Language Processing (NLP) App to summarize text using:\n",
        "    \tNLTK WordFrequency Algorithm, TextRank, and BERT. \n",
        "    \t\"\"\")\n",
        "\n",
        "\t# Word Clouds\n",
        "\tif st.checkbox(\"Generate WordCloud\"):\n",
        "\t\tst.subheader(\"Generate WordCloud from Text\")\n",
        "\n",
        "\t\tmessage = st.text_area(\"Enter Text\",\"Type Here\")\n",
        "\t\tif st.button(\"Generate\"):\n",
        "\t\t\twc_result = wordcloud(message)\n",
        "\t\t\t# st.json(wc_result)\n",
        "\n",
        "\t# Entity Extraction\n",
        "\tif st.checkbox(\"Extract Entities\"):\n",
        "\t\tst.subheader(\"Extract List of Entities\")\n",
        "\n",
        "\t\tmessage = st.text_area(\"Enter Text\",\"Type Here\")\n",
        "\t\tif st.button(\"Extract\"):\n",
        "\t\t\tentity_result = entityanalyzer(message)\n",
        "\t\t\tst.json(entity_result)\n",
        "   \n",
        "\t# Summarization\n",
        "\tif st.checkbox(\"Generate Summary\"):\n",
        "\t\tst.subheader(\"Summarize Your Text\")\n",
        "\n",
        "\t\tmessage = st.text_area(\"Enter Text\",\"Type Here\")\n",
        "\t\tsummary_options = st.selectbox(\"Choose Summarizer\",['WordFrequency','TextRank','BERT'])\n",
        "\t\tif st.button(\"Summarize\"):\n",
        "\t\t\tif summary_options == 'WordFrequency':\n",
        "\t\t\t\tst.text(\"Using WordFrequency Algorithm...\")\n",
        "\t\t\t\tsummary_result = wordfreq(message)\n",
        "\t\t\telif summary_options == 'TextRank':\n",
        "\t\t\t\tst.text(\"Using TextRank...\")\n",
        "\t\t\t\tsummary_result = textrank(message)\n",
        "\t\t\telif summary_options == 'BERT':\n",
        "\t\t\t\tst.text(\"Using BERT Summarizer...\")\n",
        "\t\t\t\tsummary_result = bert(message)\n",
        "\t\t\telse:\n",
        "\t\t\t\tst.warning(\"Using default summarizer\")\n",
        "\t\t\t\tst.text(\"Using WordFrequency Algorithm...\")\n",
        "\t\t\t\tsummary_result = summarize(message)\n",
        "\n",
        "\t\t\tst.success(summary_result)\n",
        "\n",
        "\t\tst.subheader(\"Word Counts\")\n",
        "\t\ttry:\n",
        "\t\t\twctext = wordcount(message)\n",
        "\t\t\twcsum = wordcount(summary_result)\n",
        "\t\t\tst.write('Text word count:', wctext)\n",
        "\t\t\tst.write('Summary word count:', wcsum)\n",
        "\t\texcept:\n",
        "\t\t\tpass\n",
        "\n",
        "  # sidebar\n",
        "\tst.sidebar.subheader(\"About App\")\n",
        "\tst.sidebar.text(\"Streamlit App x Google Colab\")\n",
        "\tst.sidebar.info(\"Thinkful | Data Science Flex\")\n",
        "\t\n",
        "\n",
        "\tst.sidebar.subheader(\"By\")\n",
        "\tst.sidebar.text(\"Tracie Mae Umali\")\n",
        "\tst.sidebar.text(\"https://mobdoe.github.io/\")\n",
        "\t\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tmain()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing textsummarizer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4pI_GNgZJ0J",
        "colab_type": "text"
      },
      "source": [
        "Include word counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxmzniGiVBvu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f9463d72-974f-405d-af4b-5e3c2bb2d533"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip  textsummarizer.py\n",
            "glove.6B.200d.txt  glove.6B.50d.txt   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TImAQC5HVJeb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b136ee01-468f-4041-fb0f-518d02437e0c"
      },
      "source": [
        "!nohub streamlit run app.py &"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: nohub: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w4oFABSVMav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!streamlit run textsummarizer.py &>/dev/null& # run streamlit in background"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG-FOfZTVRfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyngrok import ngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEPiLxOWVTBU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7a14e067-f64a-42e2-9038-a6764d76fe65"
      },
      "source": [
        "# Setup a tunnel to the streamlit port 8501\n",
        "public_url = ngrok.connect(port='8501')\n",
        "public_url"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'http://e3fdba3f71b4.ngrok.io'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}